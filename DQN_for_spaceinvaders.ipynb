{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPpccrjZwJhyaqtEFNR/GlN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akiran703/DQN_demo_for_SI/blob/main/DQN_for_spaceinvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]\n",
        "!pip install stable-baselines3[extra]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kLDMTBoYemAq",
        "outputId": "d7e78a4a-2e02-446d-8d0d-f11875bd3c21"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.4.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2024.2.2)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.3.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.2/182.2 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra])\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (1.25.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.0.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (3.7.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (4.66.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (13.7.1)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra])\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (9.4.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license]~=0.6.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.31.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (0.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra])\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra])\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.62.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.20.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->stable-baselines3[extra])\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (24.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->stable-baselines3[extra]) (2.16.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.8.1->shimmy[atari]~=1.3.0->stable-baselines3[extra]) (6.4.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->stable-baselines3[extra]) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard>=2.9.1->stable-baselines3[extra]) (3.2.2)\n",
            "Installing collected packages: farama-notifications, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, gymnasium, ale-py, shimmy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, stable-baselines3\n",
            "  Attempting uninstall: ale-py\n",
            "    Found existing installation: ale-py 0.7.5\n",
            "    Uninstalling ale-py-0.7.5:\n",
            "      Successfully uninstalled ale-py-0.7.5\n",
            "Successfully installed ale-py-0.8.1 farama-notifications-0.0.4 gymnasium-0.29.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 shimmy-1.3.0 stable-baselines3-2.3.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ale_py"
                ]
              },
              "id": "a868c75fcdf143c4971149d4edb7d2c6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from gym import spaces\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "\n",
        "cv2.ocl.setUseOpenCL(False)\n",
        "\n",
        "\n",
        "import stable_baselines3\n"
      ],
      "metadata": {
        "id": "cXIduZz6emHX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ale_py import ALEInterface\n",
        "ale = ALEInterface()\n",
        "\n",
        "from ale_py.roms import SpaceInvaders\n",
        "ale.loadROM(SpaceInvaders)"
      ],
      "metadata": {
        "id": "6V0RZfjye4sI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "683c2355-f1bc-4034-d5b9-6413f565938d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#in order to reduce instability for when the Deep Q-learning is being trained, we apply a replay system to help.\n",
        "#A replay system essentially is a way to store transistions that the agent observes and it can be used later training.\n",
        "#This helps with reducing catostraphic forgetting in which the agent forgets all previous experiences as its new experience and reduces correlation between experiences.\n",
        "\n",
        "\n",
        "\n",
        "#creating that class that creates the Replay system\n",
        "class ReplayMemory:\n",
        "    #constructor creating a list to store the experiences\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = []\n",
        "        self.maxlen=capacity\n",
        "        self.nextindex = 0\n",
        "\n",
        "    # we can not keep every single transition, so we will remove the oldest one\n",
        "    def push(self, state,action,reward,next_state,done):\n",
        "\n",
        "      data = (state, action, reward, next_state, done)\n",
        "\n",
        "      if self.nextindex >= len(self.memory):\n",
        "        self.memory.append(data)\n",
        "      else:\n",
        "        self.memory[self.nextindex] = data\n",
        "      self.nextindex = (self.nextindex + 1) % self.maxlen\n",
        "\n",
        "\n",
        "    #encode the sample into arrays\n",
        "    def encode_sample(self,indices):\n",
        "      states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "      for i in indices:\n",
        "        d = self.memory[i]\n",
        "        state, action, reward, next_state, done = d\n",
        "        states.append(np.array(state,copy=False))\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        next_states.append(np.array(next_state, copy=False))\n",
        "        dones.append(done)\n",
        "      return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # getting a random experience\n",
        "    def sample(self, batch_size):\n",
        "      indx = np.random.randint(0, len(self.memory) - 1, size = batch_size)\n",
        "      return self.encode_sample(indx)\n",
        "\n",
        "\n",
        "    # the length of the memory\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ],
      "metadata": {
        "id": "s7iZ3HHtw90C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self,observation_space: spaces.Box, action_space: spaces.discrete):\n",
        "     super().__init__()\n",
        "     # checks to enable that the observation space is a box, it forms a channels x widith x height, the action space is discrete\n",
        "     assert type(\n",
        "            observation_space) == spaces.Box, 'observation_space must be of type Box'\n",
        "     assert len(\n",
        "            observation_space.shape) == 3, 'observation space must have the form channels x width x height'\n",
        "     assert type(\n",
        "            action_space) == spaces.Discrete, 'action_space must be of type Discrete'\n",
        "\n",
        "      #Convolution layer with activation function\n",
        "     self.conv = nn.Sequential(\n",
        "         nn.Conv2d(in_channels=observation_space.shape[0], out_channels=32, kernel_size=8, stride=4),\n",
        "         nn.ReLU(),\n",
        "         nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "         nn.ReLU(),\n",
        "         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "         nn.ReLU()\n",
        "     )\n",
        "\n",
        "     #fully connected layer\n",
        "     self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features=64*7*7 , out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=512, out_features=action_space.n)\n",
        "        )\n",
        "  #feed foward neural network\n",
        "  def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0],-1)\n",
        "        return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "AXtroIYrw92m"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the agent class which contains double DQN and\n",
        "\n",
        "class DQN_Agent:\n",
        "  def __init__(self,observation_space: spaces.Box, action_space: spaces.discrete,replay_buffer: ReplayMemory,use_double_dqn,lr,batch_size,gamma,device=torch.device('cpu')):\n",
        "    self.memory = replay_buffer\n",
        "    #double dqn to make sure the estimated Q-value isnt overestimated\n",
        "    self.use_double_dqn = use_double_dqn\n",
        "    self.batch_size = batch_size\n",
        "    self.gamma = gamma\n",
        "\n",
        "\n",
        "\n",
        "    self.policy_network = DQN(observation_space,action_space).to(device)\n",
        "    self.target_network = DQN(observation_space,action_space).to(device)\n",
        "    self.update_target_network()\n",
        "    self.target_network.eval()\n",
        "\n",
        "\n",
        "    #in order minimize our loss function between the estimated Q-value and Q-target value, we use gradient decsent with Root Mean Square propagation\n",
        "    #gradient decsent with Root Mean Square propagation esstentially takes the learning rate and expotentially decays it when the squared gradient is less than a threshold\n",
        "    #we do this to ensure each parameter in the model is adjusted thus providing better peformance\n",
        "    #step 1: set the parameters Learning rate: α, Exponential decay rate for averaging: γ, Small constant for numerical stability: ε, Initial parameter values: θ\n",
        "    #step 2: initalize the accumulated gradients\n",
        "    #step 3 update the parameters after calculating the gradient of the objective function with respect to the parameters and updating exponentially weighted average of the squared gradients\n",
        "    self.optimiser = torch.optim.RMSprop(self.policy_network.parameters()\n",
        "            , lr=lr)\n",
        "\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "  def update_target_network(self):\n",
        "\n",
        "    self.target_network.load_state_dict(self.policy_network.state_dict())\n",
        "\n",
        "  def temporaldifference_loss(self):\n",
        "    device = self.device\n",
        "\n",
        "\n",
        "    states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n",
        "    states = np.array(states) / 255.0\n",
        "    next_states = np.array(next_states) / 255.0\n",
        "    states = torch.from_numpy(states).float().to(device)\n",
        "    actions = torch.from_numpy(actions).long().to(device)\n",
        "    rewards = torch.from_numpy(rewards).float().to(device)\n",
        "    next_states = torch.from_numpy(next_states).float().to(device)\n",
        "    dones = torch.from_numpy(dones).float().to(device)\n",
        "\n",
        "\n",
        "    #double DQN uses to networks, a target network and a policy network\n",
        "    # policy network will choose the highest Q-value and that will be our action\n",
        "    # target network to calculate the  Q-value of taking that action in the next state\n",
        "    with torch.no_grad():\n",
        "      _, max_next_action = self.policy_network(next_states).max(1)\n",
        "      estimated_q_value = self.target_network(next_states).gather(1,max_next_action.unsqueeze(1)).squeeze()\n",
        "      target_q_value = rewards + (1-dones) * self.gamma * (estimated_q_value)\n",
        "\n",
        "    pre_q_value = self.policy_network(states)\n",
        "    pre_q_value = pre_q_value.gather(1,actions.unsqueeze(1)).squeeze()\n",
        "\n",
        "    td_loss = F.smooth_l1_loss(pre_q_value, target_q_value)\n",
        "\n",
        "\n",
        "    self.optimiser.zero_grad()\n",
        "    td_loss.backward()\n",
        "    self.optimiser.step()\n",
        "    del states\n",
        "    del next_states\n",
        "    return td_loss.item()\n",
        "\n",
        "  def act(self,state: np.ndarray):\n",
        "    #using greedy policy to choose the action\n",
        "\n",
        "    device = self.device\n",
        "    state = np.array(state) / 255.0\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "      q_values = self.policy_network(state)\n",
        "      _, action = q_values.max(1)\n",
        "      return action.item()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LChc70SSw94X"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoopResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env, noop_max=30):\n",
        "        \"\"\"Sample initial states by taking random number of no-ops on reset.\n",
        "        No-op is assumed to be action 0.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.noop_max = noop_max\n",
        "        self.override_num_noops = None\n",
        "        self.noop_action = 0\n",
        "        assert env.unwrapped.get_action_meanings()[0] == 'NOOP'\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"\n",
        "        self.env.reset(**kwargs)\n",
        "        if self.override_num_noops is not None:\n",
        "            noops = self.override_num_noops\n",
        "        else:\n",
        "            noops = self.unwrapped.np_random.integers(1, self.noop_max + 1) #pylint: disable=E1101\n",
        "        assert noops > 0\n",
        "        obs = None\n",
        "        for _ in range(noops):\n",
        "            obs, _, done, info = self.env.step(self.noop_action)\n",
        "            if done:\n",
        "                obs = self.env.reset(**kwargs)\n",
        "        return obs, info\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class FireResetEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n",
        "        assert len(env.unwrapped.get_action_meanings()) >= 3\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(1)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        obs, _, done, _ = self.env.step(2)\n",
        "        if done:\n",
        "            self.env.reset(**kwargs)\n",
        "        return obs\n",
        "\n",
        "    def step(self, ac):\n",
        "        return self.env.step(ac)\n",
        "\n",
        "\n",
        "class EpisodicLifeEnv(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        Done by DeepMind for the DQN and co. since it helps value estimation.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.lives = 0\n",
        "        self.was_real_done = True\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        self.was_real_done = done\n",
        "        # check current lives, make loss of life terminal,\n",
        "        # then update lives to handle bonus lives\n",
        "        lives = self.env.unwrapped.ale.lives()\n",
        "        if lives < self.lives and lives > 0:\n",
        "            # for Qbert sometimes we stay in lives == 0 condtion for a few frames\n",
        "            # so its important to keep lives > 0, so that we only reset once\n",
        "            # the environment advertises done.\n",
        "            done = True\n",
        "        self.lives = lives\n",
        "        return obs, reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset only when lives are exhausted.\n",
        "        This way all states are still reachable even though lives are episodic,\n",
        "        and the learner need not know about any of this behind-the-scenes.\n",
        "        \"\"\"\n",
        "        if self.was_real_done:\n",
        "            obs = self.env.reset(**kwargs)\n",
        "        else:\n",
        "            # no-op step to advance from terminal/lost life state\n",
        "            obs, _, _, _ = self.env.step(0)\n",
        "        self.lives = self.env.unwrapped.ale.lives()\n",
        "        return obs\n",
        "\n",
        "\n",
        "class MaxAndSkipEnv(gym.Wrapper):\n",
        "    def __init__(self, env, skip=4):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        # most recent raw observations (for max pooling across time steps)\n",
        "        self._obs_buffer = np.zeros((2,) + env.observation_space.shape, dtype=np.uint8)\n",
        "        self._skip = skip\n",
        "\n",
        "    def reset(self):\n",
        "        return self.env.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n",
        "        total_reward = 0.0\n",
        "        done = None\n",
        "        for i in range(self._skip):\n",
        "            obs, reward, done, info = self.env.step(action)\n",
        "            if i == self._skip - 2: self._obs_buffer[0] = obs\n",
        "            if i == self._skip - 1: self._obs_buffer[1] = obs\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        # Note that the observation on the done=True frame\n",
        "        # doesn't matter\n",
        "        max_frame = self._obs_buffer.max(axis=0)\n",
        "\n",
        "        return max_frame, total_reward, done, info\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "\n",
        "class ClipRewardEnv(gym.RewardWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.RewardWrapper.__init__(self, env)\n",
        "\n",
        "    def reward(self, reward):\n",
        "        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n",
        "        return np.sign(reward)\n",
        "\n",
        "\n",
        "class WarpFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\n",
        "        Expects inputs to be of shape height x width x num_channels\n",
        "        \"\"\"\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "        self.width = 84\n",
        "        self.height = 84\n",
        "        self.observation_space = spaces.Box(low=0, high=255,\n",
        "                                            shape=(self.height, self.width, 1), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, frame):\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
        "        return frame[:, :, None]\n",
        "\n",
        "\n",
        "class FrameStack(gym.Wrapper):\n",
        "    def __init__(self, env, k):\n",
        "        \"\"\"Stack k last frames.\n",
        "        Returns lazy array, which is much more memory efficient.\n",
        "        Expects inputs to be of shape num_channels x height x width.\n",
        "        \"\"\"\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.k = k\n",
        "        self.frames = deque([], maxlen=k)\n",
        "        shp = env.observation_space.shape\n",
        "        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0] * k, shp[1], shp[2]), dtype=np.uint8)\n",
        "\n",
        "    def reset(self):\n",
        "        ob = self.env.reset()\n",
        "        for _ in range(self.k):\n",
        "            self.frames.append(ob)\n",
        "        return self._get_ob()\n",
        "\n",
        "    def step(self, action):\n",
        "        ob, reward, done, info = self.env.step(action)\n",
        "        self.frames.append(ob)\n",
        "        return self._get_ob(), reward, done, info\n",
        "\n",
        "    def _get_ob(self):\n",
        "        assert len(self.frames) == self.k\n",
        "        return LazyFrames(list(self.frames))\n",
        "\n",
        "\n",
        "class ScaledFloatFrame(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        gym.ObservationWrapper.__init__(self, env)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        # careful! This undoes the memory optimization, use\n",
        "        # with smaller replay buffers only.\n",
        "        return np.array(observation).astype(np.float32) / 255.0\n",
        "\n",
        "\n",
        "class LazyFrames(object):\n",
        "    def __init__(self, frames):\n",
        "        \"\"\"This object ensures that common frames between the observations are only stored once.\n",
        "        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n",
        "        buffers.\"\"\"\n",
        "        self._frames = frames\n",
        "\n",
        "    def __array__(self, dtype=None):\n",
        "        out = np.concatenate(self._frames, axis=0)\n",
        "        if dtype is not None:\n",
        "            out = out.astype(dtype)\n",
        "        return out\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._frames)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        return self._frames[i]\n",
        "\n",
        "\n",
        "class PyTorchFrame(gym.ObservationWrapper):\n",
        "    \"\"\"Image shape to num_channels x height x width\"\"\"\n",
        "\n",
        "    def __init__(self, env):\n",
        "        super(PyTorchFrame, self).__init__(env)\n",
        "        shape = self.observation_space.shape\n",
        "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(shape[-1], shape[0], shape[1]), dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.rollaxis(observation, 2)"
      ],
      "metadata": {
        "id": "u9NFbkkGkvD1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == '__main__':\n",
        "    hyper_params = {\n",
        "        \"seed\": 1,  # which seed to use\n",
        "        \"env\": \"ALE/SpaceInvaders-v5\",  # name of the game\n",
        "        \"replay-buffer-size\": int(5e3),  # replay buffer size\n",
        "        \"learning-rate\": 1e-4,  # learning rate for Adam optimizer\n",
        "        \"discount-factor\": 0.99,  # discount factor\n",
        "        \"num-steps\": int(1e6),   # total number of steps to run the environment for\n",
        "        \"batch-size\": 32,  # number of transitions to optimize at the same time\n",
        "        \"learning-starts\": 10000,  # number of steps before learning starts\n",
        "        \"learning-freq\": 1,  # number of iterations between every optimization step\n",
        "        \"use-double-dqn\": True,  # use double deep Q-learning\n",
        "        \"target-update-freq\": 1000,  # number of iterations between every target network update\n",
        "        \"eps-start\": 1,  # e-greedy start threshold\n",
        "        \"eps-end\": 0.01,  # e-greedy end threshold\n",
        "        \"eps-fraction\": 0.1,  # fraction of num-steps\n",
        "        \"print-freq\": 10\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    np.random.seed(hyper_params['seed'])\n",
        "    random.seed(hyper_params['seed'])\n",
        "\n",
        "\n",
        "    #make env\n",
        "    env = gym.make(hyper_params['env'])\n",
        "    env.seed(hyper_params['seed'])\n",
        "\n",
        "\n",
        "    # sample inital states by taking random number of no-ops on reset\n",
        "    env = NoopResetEnv(env, noop_max=30)\n",
        "    # return every nth-skip frame\n",
        "    env = MaxAndSkipEnv(env, skip=4)\n",
        "    # make end-of-life == end-of-episode\n",
        "    env = EpisodicLifeEnv(env)\n",
        "    # take action on reset for env that are fixed until firing\n",
        "    env = FireResetEnv(env)\n",
        "    # make dimensions into 84 x 84 and make rbg to gray\n",
        "    env = WarpFrame(env)\n",
        "    # image shape to num_channels x height x widith\n",
        "    env = PyTorchFrame(env)\n",
        "    #[1,0,-1]\n",
        "    env = ClipRewardEnv(env)\n",
        "    #get the last k frames and stack\n",
        "    env = FrameStack(env, 4)\n",
        "\n",
        "\n",
        "    replay_buffer = ReplayMemory(hyper_params['replay-buffer-size'])\n",
        "\n",
        "    agent = DQN_Agent(\n",
        "        env.observation_space,\n",
        "        env.action_space,\n",
        "        replay_buffer,\n",
        "        use_double_dqn=hyper_params[\"use-double-dqn\"],\n",
        "        lr=hyper_params['learning-rate'],\n",
        "        batch_size=hyper_params['batch-size'],\n",
        "        gamma=hyper_params['discount-factor'],\n",
        "        device=torch.device(\"cpu\")\n",
        "    )\n",
        "\n",
        "\n",
        "    eps_timesteps = hyper_params[\"eps-fraction\"] * \\\n",
        "        float(hyper_params[\"num-steps\"])\n",
        "    episode_rewards = [0.0]\n",
        "\n",
        "\n",
        "    state = env.reset()\n",
        "    for t in range(hyper_params[\"num-steps\"]):\n",
        "        fraction = min(1.0, float(t) / eps_timesteps)\n",
        "        eps_threshold = hyper_params[\"eps-start\"] + fraction * \\\n",
        "            (hyper_params[\"eps-end\"] - hyper_params[\"eps-start\"])\n",
        "        sample = random.random()\n",
        "\n",
        "        if(sample > eps_threshold):\n",
        "            # Exploit\n",
        "            action = agent.act(state)\n",
        "        else:\n",
        "            # Explore\n",
        "            action = env.action_space.sample()\n",
        "\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "        agent.memory.push(state, action, reward, next_state, float(done))\n",
        "        state = next_state\n",
        "\n",
        "        episode_rewards[-1] += reward\n",
        "        if done:\n",
        "            state = env.reset()\n",
        "            episode_rewards.append(0.0)\n",
        "\n",
        "        if t > hyper_params[\"learning-starts\"] and t % hyper_params[\"learning-freq\"] == 0:\n",
        "            agent.temporaldifference_loss()\n",
        "\n",
        "        if t > hyper_params[\"learning-starts\"] and t % hyper_params[\"target-update-freq\"] == 0:\n",
        "            agent.update_target_network()\n",
        "\n",
        "        num_episodes = len(episode_rewards)\n",
        "\n",
        "        if done and hyper_params[\"print-freq\"] is not None and len(episode_rewards) % hyper_params[\n",
        "                \"print-freq\"] == 0:\n",
        "            mean_100ep_reward = round(np.mean(episode_rewards[-101:-1]), 1)\n",
        "            print(\"********************************************************\")\n",
        "            print(\"steps: {}\".format(t))\n",
        "            print(\"episodes: {}\".format(num_episodes))\n",
        "            print(\"mean 100 episode reward: {}\".format(mean_100ep_reward))\n",
        "            print(\"% time spent exploring: {}\".format(int(100 * eps_threshold)))\n",
        "            print(\"********************************************************\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEvRpGJBfLP1",
        "outputId": "88166ba4-cc82-4fa2-9d29-f73a854e861a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "********************************************************\n",
            "steps: 225\n",
            "episodes: 10\n",
            "mean 100 episode reward: 2.6\n",
            "% time spent exploring: 99\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 595\n",
            "episodes: 20\n",
            "mean 100 episode reward: 3.2\n",
            "% time spent exploring: 99\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 883\n",
            "episodes: 30\n",
            "mean 100 episode reward: 2.7\n",
            "% time spent exploring: 99\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 1356\n",
            "episodes: 40\n",
            "mean 100 episode reward: 3.4\n",
            "% time spent exploring: 98\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 1915\n",
            "episodes: 50\n",
            "mean 100 episode reward: 3.9\n",
            "% time spent exploring: 98\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 2331\n",
            "episodes: 60\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 97\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 2743\n",
            "episodes: 70\n",
            "mean 100 episode reward: 3.6\n",
            "% time spent exploring: 97\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 3080\n",
            "episodes: 80\n",
            "mean 100 episode reward: 3.5\n",
            "% time spent exploring: 96\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 3369\n",
            "episodes: 90\n",
            "mean 100 episode reward: 3.4\n",
            "% time spent exploring: 96\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 3598\n",
            "episodes: 100\n",
            "mean 100 episode reward: 3.2\n",
            "% time spent exploring: 96\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 4189\n",
            "episodes: 110\n",
            "mean 100 episode reward: 3.5\n",
            "% time spent exploring: 95\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 4540\n",
            "episodes: 120\n",
            "mean 100 episode reward: 3.4\n",
            "% time spent exploring: 95\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 4973\n",
            "episodes: 130\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 95\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 5311\n",
            "episodes: 140\n",
            "mean 100 episode reward: 3.3\n",
            "% time spent exploring: 94\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 5771\n",
            "episodes: 150\n",
            "mean 100 episode reward: 3.2\n",
            "% time spent exploring: 94\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 6051\n",
            "episodes: 160\n",
            "mean 100 episode reward: 3.1\n",
            "% time spent exploring: 94\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 6518\n",
            "episodes: 170\n",
            "mean 100 episode reward: 3.2\n",
            "% time spent exploring: 93\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 6937\n",
            "episodes: 180\n",
            "mean 100 episode reward: 3.2\n",
            "% time spent exploring: 93\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 7152\n",
            "episodes: 190\n",
            "mean 100 episode reward: 3.2\n",
            "% time spent exploring: 92\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 7626\n",
            "episodes: 200\n",
            "mean 100 episode reward: 3.3\n",
            "% time spent exploring: 92\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 8140\n",
            "episodes: 210\n",
            "mean 100 episode reward: 3.2\n",
            "% time spent exploring: 91\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 8537\n",
            "episodes: 220\n",
            "mean 100 episode reward: 3.2\n",
            "% time spent exploring: 91\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 8935\n",
            "episodes: 230\n",
            "mean 100 episode reward: 3.0\n",
            "% time spent exploring: 91\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 9285\n",
            "episodes: 240\n",
            "mean 100 episode reward: 3.0\n",
            "% time spent exploring: 90\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 9635\n",
            "episodes: 250\n",
            "mean 100 episode reward: 2.8\n",
            "% time spent exploring: 90\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 9875\n",
            "episodes: 260\n",
            "mean 100 episode reward: 2.8\n",
            "% time spent exploring: 90\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 10284\n",
            "episodes: 270\n",
            "mean 100 episode reward: 2.8\n",
            "% time spent exploring: 89\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 10605\n",
            "episodes: 280\n",
            "mean 100 episode reward: 2.8\n",
            "% time spent exploring: 89\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 11080\n",
            "episodes: 290\n",
            "mean 100 episode reward: 3.0\n",
            "% time spent exploring: 89\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 11519\n",
            "episodes: 300\n",
            "mean 100 episode reward: 3.0\n",
            "% time spent exploring: 88\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 11864\n",
            "episodes: 310\n",
            "mean 100 episode reward: 2.9\n",
            "% time spent exploring: 88\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 12230\n",
            "episodes: 320\n",
            "mean 100 episode reward: 2.9\n",
            "% time spent exploring: 87\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 12586\n",
            "episodes: 330\n",
            "mean 100 episode reward: 2.9\n",
            "% time spent exploring: 87\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 12931\n",
            "episodes: 340\n",
            "mean 100 episode reward: 2.9\n",
            "% time spent exploring: 87\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 13312\n",
            "episodes: 350\n",
            "mean 100 episode reward: 2.9\n",
            "% time spent exploring: 86\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 13642\n",
            "episodes: 360\n",
            "mean 100 episode reward: 2.9\n",
            "% time spent exploring: 86\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 14061\n",
            "episodes: 370\n",
            "mean 100 episode reward: 2.9\n",
            "% time spent exploring: 86\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 14441\n",
            "episodes: 380\n",
            "mean 100 episode reward: 2.9\n",
            "% time spent exploring: 85\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 14830\n",
            "episodes: 390\n",
            "mean 100 episode reward: 2.8\n",
            "% time spent exploring: 85\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 15329\n",
            "episodes: 400\n",
            "mean 100 episode reward: 2.9\n",
            "% time spent exploring: 84\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 15768\n",
            "episodes: 410\n",
            "mean 100 episode reward: 3.0\n",
            "% time spent exploring: 84\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 16153\n",
            "episodes: 420\n",
            "mean 100 episode reward: 3.0\n",
            "% time spent exploring: 84\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 16507\n",
            "episodes: 430\n",
            "mean 100 episode reward: 3.1\n",
            "% time spent exploring: 83\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 16873\n",
            "episodes: 440\n",
            "mean 100 episode reward: 3.1\n",
            "% time spent exploring: 83\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 17173\n",
            "episodes: 450\n",
            "mean 100 episode reward: 3.1\n",
            "% time spent exploring: 82\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 17755\n",
            "episodes: 460\n",
            "mean 100 episode reward: 3.5\n",
            "% time spent exploring: 82\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 18265\n",
            "episodes: 470\n",
            "mean 100 episode reward: 3.5\n",
            "% time spent exploring: 81\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 18636\n",
            "episodes: 480\n",
            "mean 100 episode reward: 3.6\n",
            "% time spent exploring: 81\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 18995\n",
            "episodes: 490\n",
            "mean 100 episode reward: 3.6\n",
            "% time spent exploring: 81\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 19375\n",
            "episodes: 500\n",
            "mean 100 episode reward: 3.5\n",
            "% time spent exploring: 80\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 19715\n",
            "episodes: 510\n",
            "mean 100 episode reward: 3.4\n",
            "% time spent exploring: 80\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 20203\n",
            "episodes: 520\n",
            "mean 100 episode reward: 3.6\n",
            "% time spent exploring: 79\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 20656\n",
            "episodes: 530\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 79\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 21107\n",
            "episodes: 540\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 79\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 21367\n",
            "episodes: 550\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 78\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 21777\n",
            "episodes: 560\n",
            "mean 100 episode reward: 3.5\n",
            "% time spent exploring: 78\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 22261\n",
            "episodes: 570\n",
            "mean 100 episode reward: 3.5\n",
            "% time spent exploring: 77\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 22744\n",
            "episodes: 580\n",
            "mean 100 episode reward: 3.6\n",
            "% time spent exploring: 77\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 23108\n",
            "episodes: 590\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 77\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 23445\n",
            "episodes: 600\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 76\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 23795\n",
            "episodes: 610\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 76\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 24280\n",
            "episodes: 620\n",
            "mean 100 episode reward: 3.5\n",
            "% time spent exploring: 75\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 24592\n",
            "episodes: 630\n",
            "mean 100 episode reward: 3.4\n",
            "% time spent exploring: 75\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 25028\n",
            "episodes: 640\n",
            "mean 100 episode reward: 3.5\n",
            "% time spent exploring: 75\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 25507\n",
            "episodes: 650\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 74\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 25948\n",
            "episodes: 660\n",
            "mean 100 episode reward: 3.9\n",
            "% time spent exploring: 74\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 26404\n",
            "episodes: 670\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 73\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 26788\n",
            "episodes: 680\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 73\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 27135\n",
            "episodes: 690\n",
            "mean 100 episode reward: 3.6\n",
            "% time spent exploring: 73\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 27544\n",
            "episodes: 700\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 72\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 27849\n",
            "episodes: 710\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 72\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 28255\n",
            "episodes: 720\n",
            "mean 100 episode reward: 3.6\n",
            "% time spent exploring: 72\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 28686\n",
            "episodes: 730\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 71\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 29057\n",
            "episodes: 740\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 71\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 29401\n",
            "episodes: 750\n",
            "mean 100 episode reward: 3.5\n",
            "% time spent exploring: 70\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 29871\n",
            "episodes: 760\n",
            "mean 100 episode reward: 3.5\n",
            "% time spent exploring: 70\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 30187\n",
            "episodes: 770\n",
            "mean 100 episode reward: 3.4\n",
            "% time spent exploring: 70\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 30645\n",
            "episodes: 780\n",
            "mean 100 episode reward: 3.6\n",
            "% time spent exploring: 69\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 31089\n",
            "episodes: 790\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 69\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 31532\n",
            "episodes: 800\n",
            "mean 100 episode reward: 3.9\n",
            "% time spent exploring: 68\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 31792\n",
            "episodes: 810\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 68\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 32091\n",
            "episodes: 820\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 68\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 32455\n",
            "episodes: 830\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 67\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 32866\n",
            "episodes: 840\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 67\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 33254\n",
            "episodes: 850\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 67\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 33747\n",
            "episodes: 860\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 66\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 34171\n",
            "episodes: 870\n",
            "mean 100 episode reward: 3.9\n",
            "% time spent exploring: 66\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 34742\n",
            "episodes: 880\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 65\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 35070\n",
            "episodes: 890\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 65\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 35457\n",
            "episodes: 900\n",
            "mean 100 episode reward: 3.7\n",
            "% time spent exploring: 64\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 35866\n",
            "episodes: 910\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 64\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 36257\n",
            "episodes: 920\n",
            "mean 100 episode reward: 4.0\n",
            "% time spent exploring: 64\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 36683\n",
            "episodes: 930\n",
            "mean 100 episode reward: 4.0\n",
            "% time spent exploring: 63\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 37096\n",
            "episodes: 940\n",
            "mean 100 episode reward: 4.0\n",
            "% time spent exploring: 63\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 37540\n",
            "episodes: 950\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 62\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 38063\n",
            "episodes: 960\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 62\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 38432\n",
            "episodes: 970\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 61\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 38798\n",
            "episodes: 980\n",
            "mean 100 episode reward: 4.1\n",
            "% time spent exploring: 61\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 39220\n",
            "episodes: 990\n",
            "mean 100 episode reward: 4.0\n",
            "% time spent exploring: 61\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 39853\n",
            "episodes: 1000\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 60\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 40104\n",
            "episodes: 1010\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 60\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 40732\n",
            "episodes: 1020\n",
            "mean 100 episode reward: 4.3\n",
            "% time spent exploring: 59\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 41202\n",
            "episodes: 1030\n",
            "mean 100 episode reward: 4.4\n",
            "% time spent exploring: 59\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 41636\n",
            "episodes: 1040\n",
            "mean 100 episode reward: 4.4\n",
            "% time spent exploring: 58\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 41913\n",
            "episodes: 1050\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 58\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 42415\n",
            "episodes: 1060\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 58\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 42802\n",
            "episodes: 1070\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 57\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 43277\n",
            "episodes: 1080\n",
            "mean 100 episode reward: 4.3\n",
            "% time spent exploring: 57\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 43524\n",
            "episodes: 1090\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 56\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 43900\n",
            "episodes: 1100\n",
            "mean 100 episode reward: 3.9\n",
            "% time spent exploring: 56\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 44442\n",
            "episodes: 1110\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 56\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 44953\n",
            "episodes: 1120\n",
            "mean 100 episode reward: 4.1\n",
            "% time spent exploring: 55\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 45390\n",
            "episodes: 1130\n",
            "mean 100 episode reward: 4.0\n",
            "% time spent exploring: 55\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 45709\n",
            "episodes: 1140\n",
            "mean 100 episode reward: 3.9\n",
            "% time spent exploring: 54\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 46064\n",
            "episodes: 1150\n",
            "mean 100 episode reward: 4.1\n",
            "% time spent exploring: 54\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 46457\n",
            "episodes: 1160\n",
            "mean 100 episode reward: 3.8\n",
            "% time spent exploring: 54\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 46903\n",
            "episodes: 1170\n",
            "mean 100 episode reward: 4.0\n",
            "% time spent exploring: 53\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 47265\n",
            "episodes: 1180\n",
            "mean 100 episode reward: 4.0\n",
            "% time spent exploring: 53\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 47714\n",
            "episodes: 1190\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 52\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 48146\n",
            "episodes: 1200\n",
            "mean 100 episode reward: 4.3\n",
            "% time spent exploring: 52\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 48549\n",
            "episodes: 1210\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 51\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 48939\n",
            "episodes: 1220\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 51\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 49441\n",
            "episodes: 1230\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 51\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 49791\n",
            "episodes: 1240\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 50\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 50233\n",
            "episodes: 1250\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 50\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 50712\n",
            "episodes: 1260\n",
            "mean 100 episode reward: 4.3\n",
            "% time spent exploring: 49\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 51160\n",
            "episodes: 1270\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 49\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 51543\n",
            "episodes: 1280\n",
            "mean 100 episode reward: 4.1\n",
            "% time spent exploring: 48\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 52024\n",
            "episodes: 1290\n",
            "mean 100 episode reward: 4.1\n",
            "% time spent exploring: 48\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 52516\n",
            "episodes: 1300\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 48\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 52840\n",
            "episodes: 1310\n",
            "mean 100 episode reward: 4.0\n",
            "% time spent exploring: 47\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 53564\n",
            "episodes: 1320\n",
            "mean 100 episode reward: 4.4\n",
            "% time spent exploring: 46\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 53879\n",
            "episodes: 1330\n",
            "mean 100 episode reward: 4.2\n",
            "% time spent exploring: 46\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 54297\n",
            "episodes: 1340\n",
            "mean 100 episode reward: 4.3\n",
            "% time spent exploring: 46\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 54689\n",
            "episodes: 1350\n",
            "mean 100 episode reward: 4.4\n",
            "% time spent exploring: 45\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 55207\n",
            "episodes: 1360\n",
            "mean 100 episode reward: 4.5\n",
            "% time spent exploring: 45\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 55691\n",
            "episodes: 1370\n",
            "mean 100 episode reward: 4.6\n",
            "% time spent exploring: 44\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 56302\n",
            "episodes: 1380\n",
            "mean 100 episode reward: 5.0\n",
            "% time spent exploring: 44\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 56845\n",
            "episodes: 1390\n",
            "mean 100 episode reward: 5.0\n",
            "% time spent exploring: 43\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 57271\n",
            "episodes: 1400\n",
            "mean 100 episode reward: 5.0\n",
            "% time spent exploring: 43\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 57777\n",
            "episodes: 1410\n",
            "mean 100 episode reward: 5.2\n",
            "% time spent exploring: 42\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 58303\n",
            "episodes: 1420\n",
            "mean 100 episode reward: 5.0\n",
            "% time spent exploring: 42\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 58729\n",
            "episodes: 1430\n",
            "mean 100 episode reward: 5.2\n",
            "% time spent exploring: 41\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 59151\n",
            "episodes: 1440\n",
            "mean 100 episode reward: 5.3\n",
            "% time spent exploring: 41\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 59558\n",
            "episodes: 1450\n",
            "mean 100 episode reward: 5.3\n",
            "% time spent exploring: 41\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 59950\n",
            "episodes: 1460\n",
            "mean 100 episode reward: 5.1\n",
            "% time spent exploring: 40\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 60446\n",
            "episodes: 1470\n",
            "mean 100 episode reward: 5.1\n",
            "% time spent exploring: 40\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 60951\n",
            "episodes: 1480\n",
            "mean 100 episode reward: 5.0\n",
            "% time spent exploring: 39\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 61456\n",
            "episodes: 1490\n",
            "mean 100 episode reward: 5.1\n",
            "% time spent exploring: 39\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 61879\n",
            "episodes: 1500\n",
            "mean 100 episode reward: 5.1\n",
            "% time spent exploring: 38\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 62246\n",
            "episodes: 1510\n",
            "mean 100 episode reward: 5.0\n",
            "% time spent exploring: 38\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 62664\n",
            "episodes: 1520\n",
            "mean 100 episode reward: 4.8\n",
            "% time spent exploring: 37\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 63095\n",
            "episodes: 1530\n",
            "mean 100 episode reward: 4.6\n",
            "% time spent exploring: 37\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 63504\n",
            "episodes: 1540\n",
            "mean 100 episode reward: 4.5\n",
            "% time spent exploring: 37\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 64156\n",
            "episodes: 1550\n",
            "mean 100 episode reward: 4.7\n",
            "% time spent exploring: 36\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 64613\n",
            "episodes: 1560\n",
            "mean 100 episode reward: 4.7\n",
            "% time spent exploring: 36\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 65094\n",
            "episodes: 1570\n",
            "mean 100 episode reward: 4.7\n",
            "% time spent exploring: 35\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 65614\n",
            "episodes: 1580\n",
            "mean 100 episode reward: 4.6\n",
            "% time spent exploring: 35\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 66058\n",
            "episodes: 1590\n",
            "mean 100 episode reward: 4.4\n",
            "% time spent exploring: 34\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 66492\n",
            "episodes: 1600\n",
            "mean 100 episode reward: 4.3\n",
            "% time spent exploring: 34\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 67016\n",
            "episodes: 1610\n",
            "mean 100 episode reward: 4.4\n",
            "% time spent exploring: 33\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 67508\n",
            "episodes: 1620\n",
            "mean 100 episode reward: 4.5\n",
            "% time spent exploring: 33\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 67873\n",
            "episodes: 1630\n",
            "mean 100 episode reward: 4.5\n",
            "% time spent exploring: 32\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 68400\n",
            "episodes: 1640\n",
            "mean 100 episode reward: 4.7\n",
            "% time spent exploring: 32\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 68841\n",
            "episodes: 1650\n",
            "mean 100 episode reward: 4.5\n",
            "% time spent exploring: 31\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 69587\n",
            "episodes: 1660\n",
            "mean 100 episode reward: 4.9\n",
            "% time spent exploring: 31\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 70275\n",
            "episodes: 1670\n",
            "mean 100 episode reward: 5.2\n",
            "% time spent exploring: 30\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 70694\n",
            "episodes: 1680\n",
            "mean 100 episode reward: 5.0\n",
            "% time spent exploring: 30\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 71327\n",
            "episodes: 1690\n",
            "mean 100 episode reward: 5.2\n",
            "% time spent exploring: 29\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 71884\n",
            "episodes: 1700\n",
            "mean 100 episode reward: 5.4\n",
            "% time spent exploring: 28\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 72324\n",
            "episodes: 1710\n",
            "mean 100 episode reward: 5.4\n",
            "% time spent exploring: 28\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 72760\n",
            "episodes: 1720\n",
            "mean 100 episode reward: 5.4\n",
            "% time spent exploring: 27\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 73296\n",
            "episodes: 1730\n",
            "mean 100 episode reward: 5.6\n",
            "% time spent exploring: 27\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 73730\n",
            "episodes: 1740\n",
            "mean 100 episode reward: 5.6\n",
            "% time spent exploring: 27\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 74212\n",
            "episodes: 1750\n",
            "mean 100 episode reward: 5.6\n",
            "% time spent exploring: 26\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 74847\n",
            "episodes: 1760\n",
            "mean 100 episode reward: 5.5\n",
            "% time spent exploring: 25\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 75257\n",
            "episodes: 1770\n",
            "mean 100 episode reward: 5.0\n",
            "% time spent exploring: 25\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 75857\n",
            "episodes: 1780\n",
            "mean 100 episode reward: 5.3\n",
            "% time spent exploring: 24\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 76280\n",
            "episodes: 1790\n",
            "mean 100 episode reward: 5.1\n",
            "% time spent exploring: 24\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 76825\n",
            "episodes: 1800\n",
            "mean 100 episode reward: 5.1\n",
            "% time spent exploring: 23\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 77321\n",
            "episodes: 1810\n",
            "mean 100 episode reward: 5.2\n",
            "% time spent exploring: 23\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 77785\n",
            "episodes: 1820\n",
            "mean 100 episode reward: 5.2\n",
            "% time spent exploring: 22\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 78294\n",
            "episodes: 1830\n",
            "mean 100 episode reward: 5.1\n",
            "% time spent exploring: 22\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 78914\n",
            "episodes: 1840\n",
            "mean 100 episode reward: 5.5\n",
            "% time spent exploring: 21\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 79316\n",
            "episodes: 1850\n",
            "mean 100 episode reward: 5.5\n",
            "% time spent exploring: 21\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 80042\n",
            "episodes: 1860\n",
            "mean 100 episode reward: 5.6\n",
            "% time spent exploring: 20\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 80526\n",
            "episodes: 1870\n",
            "mean 100 episode reward: 5.8\n",
            "% time spent exploring: 20\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 81130\n",
            "episodes: 1880\n",
            "mean 100 episode reward: 5.8\n",
            "% time spent exploring: 19\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 81729\n",
            "episodes: 1890\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 19\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 82174\n",
            "episodes: 1900\n",
            "mean 100 episode reward: 5.9\n",
            "% time spent exploring: 18\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 82671\n",
            "episodes: 1910\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 18\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 83193\n",
            "episodes: 1920\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 17\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 83728\n",
            "episodes: 1930\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 17\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 84376\n",
            "episodes: 1940\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 16\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 84862\n",
            "episodes: 1950\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 15\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 85525\n",
            "episodes: 1960\n",
            "mean 100 episode reward: 5.9\n",
            "% time spent exploring: 15\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 86202\n",
            "episodes: 1970\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 14\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 87051\n",
            "episodes: 1980\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 13\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 87522\n",
            "episodes: 1990\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 13\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 87940\n",
            "episodes: 2000\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 12\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 88607\n",
            "episodes: 2010\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 12\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 89053\n",
            "episodes: 2020\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 11\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 89460\n",
            "episodes: 2030\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 11\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 89887\n",
            "episodes: 2040\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 11\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 90432\n",
            "episodes: 2050\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 10\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 90863\n",
            "episodes: 2060\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 10\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 91204\n",
            "episodes: 2070\n",
            "mean 100 episode reward: 5.6\n",
            "% time spent exploring: 9\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 91978\n",
            "episodes: 2080\n",
            "mean 100 episode reward: 5.6\n",
            "% time spent exploring: 8\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 92461\n",
            "episodes: 2090\n",
            "mean 100 episode reward: 5.5\n",
            "% time spent exploring: 8\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 92861\n",
            "episodes: 2100\n",
            "mean 100 episode reward: 5.5\n",
            "% time spent exploring: 8\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 93370\n",
            "episodes: 2110\n",
            "mean 100 episode reward: 5.3\n",
            "% time spent exploring: 7\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 94025\n",
            "episodes: 2120\n",
            "mean 100 episode reward: 5.6\n",
            "% time spent exploring: 6\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 94465\n",
            "episodes: 2130\n",
            "mean 100 episode reward: 5.6\n",
            "% time spent exploring: 6\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 95030\n",
            "episodes: 2140\n",
            "mean 100 episode reward: 5.8\n",
            "% time spent exploring: 5\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 95545\n",
            "episodes: 2150\n",
            "mean 100 episode reward: 5.8\n",
            "% time spent exploring: 5\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 96043\n",
            "episodes: 2160\n",
            "mean 100 episode reward: 5.9\n",
            "% time spent exploring: 4\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 96564\n",
            "episodes: 2170\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 4\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 96926\n",
            "episodes: 2180\n",
            "mean 100 episode reward: 5.6\n",
            "% time spent exploring: 4\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 97501\n",
            "episodes: 2190\n",
            "mean 100 episode reward: 5.8\n",
            "% time spent exploring: 3\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 98121\n",
            "episodes: 2200\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 2\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 98777\n",
            "episodes: 2210\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 2\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 99309\n",
            "episodes: 2220\n",
            "mean 100 episode reward: 5.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 99803\n",
            "episodes: 2230\n",
            "mean 100 episode reward: 5.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 100468\n",
            "episodes: 2240\n",
            "mean 100 episode reward: 5.9\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 100970\n",
            "episodes: 2250\n",
            "mean 100 episode reward: 5.9\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 101642\n",
            "episodes: 2260\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 102049\n",
            "episodes: 2270\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 102526\n",
            "episodes: 2280\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 103178\n",
            "episodes: 2290\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 103680\n",
            "episodes: 2300\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 104175\n",
            "episodes: 2310\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 104762\n",
            "episodes: 2320\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 105277\n",
            "episodes: 2330\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 105845\n",
            "episodes: 2340\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 106296\n",
            "episodes: 2350\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 106737\n",
            "episodes: 2360\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 107420\n",
            "episodes: 2370\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 108011\n",
            "episodes: 2380\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 108672\n",
            "episodes: 2390\n",
            "mean 100 episode reward: 7.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 109115\n",
            "episodes: 2400\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 109811\n",
            "episodes: 2410\n",
            "mean 100 episode reward: 6.9\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 110267\n",
            "episodes: 2420\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 110942\n",
            "episodes: 2430\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 111722\n",
            "episodes: 2440\n",
            "mean 100 episode reward: 7.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 112347\n",
            "episodes: 2450\n",
            "mean 100 episode reward: 7.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 112977\n",
            "episodes: 2460\n",
            "mean 100 episode reward: 7.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 113597\n",
            "episodes: 2470\n",
            "mean 100 episode reward: 7.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 113927\n",
            "episodes: 2480\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 114665\n",
            "episodes: 2490\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 115260\n",
            "episodes: 2500\n",
            "mean 100 episode reward: 7.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 115730\n",
            "episodes: 2510\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 116266\n",
            "episodes: 2520\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 116825\n",
            "episodes: 2530\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 117328\n",
            "episodes: 2540\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 117966\n",
            "episodes: 2550\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 118610\n",
            "episodes: 2560\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 119091\n",
            "episodes: 2570\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 119549\n",
            "episodes: 2580\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 119955\n",
            "episodes: 2590\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 120504\n",
            "episodes: 2600\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 121247\n",
            "episodes: 2610\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 121872\n",
            "episodes: 2620\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 122511\n",
            "episodes: 2630\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 123115\n",
            "episodes: 2640\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 123741\n",
            "episodes: 2650\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 124239\n",
            "episodes: 2660\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 124720\n",
            "episodes: 2670\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 125215\n",
            "episodes: 2680\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 125942\n",
            "episodes: 2690\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 126591\n",
            "episodes: 2700\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 127212\n",
            "episodes: 2710\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 127707\n",
            "episodes: 2720\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 128158\n",
            "episodes: 2730\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 128764\n",
            "episodes: 2740\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 129200\n",
            "episodes: 2750\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 129501\n",
            "episodes: 2760\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 130082\n",
            "episodes: 2770\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 130642\n",
            "episodes: 2780\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 131252\n",
            "episodes: 2790\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 131933\n",
            "episodes: 2800\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 132614\n",
            "episodes: 2810\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 133189\n",
            "episodes: 2820\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 133674\n",
            "episodes: 2830\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 134180\n",
            "episodes: 2840\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 134694\n",
            "episodes: 2850\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 135271\n",
            "episodes: 2860\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 135683\n",
            "episodes: 2870\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 136249\n",
            "episodes: 2880\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 136878\n",
            "episodes: 2890\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 137533\n",
            "episodes: 2900\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 138209\n",
            "episodes: 2910\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 138916\n",
            "episodes: 2920\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 139437\n",
            "episodes: 2930\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 140003\n",
            "episodes: 2940\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 140559\n",
            "episodes: 2950\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 141173\n",
            "episodes: 2960\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 141798\n",
            "episodes: 2970\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 142291\n",
            "episodes: 2980\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 142895\n",
            "episodes: 2990\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 143593\n",
            "episodes: 3000\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 144183\n",
            "episodes: 3010\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 144674\n",
            "episodes: 3020\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 145101\n",
            "episodes: 3030\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 145512\n",
            "episodes: 3040\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 146043\n",
            "episodes: 3050\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 146590\n",
            "episodes: 3060\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 147165\n",
            "episodes: 3070\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 147770\n",
            "episodes: 3080\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 148453\n",
            "episodes: 3090\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 148907\n",
            "episodes: 3100\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 149504\n",
            "episodes: 3110\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 149965\n",
            "episodes: 3120\n",
            "mean 100 episode reward: 6.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 150514\n",
            "episodes: 3130\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 151079\n",
            "episodes: 3140\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 151544\n",
            "episodes: 3150\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 152007\n",
            "episodes: 3160\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 152635\n",
            "episodes: 3170\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 153288\n",
            "episodes: 3180\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 154005\n",
            "episodes: 3190\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 154667\n",
            "episodes: 3200\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 155157\n",
            "episodes: 3210\n",
            "mean 100 episode reward: 6.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 155638\n",
            "episodes: 3220\n",
            "mean 100 episode reward: 6.2\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 156517\n",
            "episodes: 3230\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 157205\n",
            "episodes: 3240\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 158022\n",
            "episodes: 3250\n",
            "mean 100 episode reward: 7.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 158512\n",
            "episodes: 3260\n",
            "mean 100 episode reward: 7.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 159002\n",
            "episodes: 3270\n",
            "mean 100 episode reward: 7.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 159546\n",
            "episodes: 3280\n",
            "mean 100 episode reward: 7.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 160196\n",
            "episodes: 3290\n",
            "mean 100 episode reward: 7.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 160709\n",
            "episodes: 3300\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 161485\n",
            "episodes: 3310\n",
            "mean 100 episode reward: 7.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 162004\n",
            "episodes: 3320\n",
            "mean 100 episode reward: 7.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 162406\n",
            "episodes: 3330\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 162896\n",
            "episodes: 3340\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 163602\n",
            "episodes: 3350\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 164440\n",
            "episodes: 3360\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 164924\n",
            "episodes: 3370\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 165545\n",
            "episodes: 3380\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 166089\n",
            "episodes: 3390\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 166653\n",
            "episodes: 3400\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 167331\n",
            "episodes: 3410\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 168054\n",
            "episodes: 3420\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 168496\n",
            "episodes: 3430\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 168941\n",
            "episodes: 3440\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 169431\n",
            "episodes: 3450\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 170109\n",
            "episodes: 3460\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 170562\n",
            "episodes: 3470\n",
            "mean 100 episode reward: 6.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 171103\n",
            "episodes: 3480\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 171702\n",
            "episodes: 3490\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 172240\n",
            "episodes: 3500\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 172816\n",
            "episodes: 3510\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 173377\n",
            "episodes: 3520\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 173922\n",
            "episodes: 3530\n",
            "mean 100 episode reward: 6.7\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 174498\n",
            "episodes: 3540\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 174950\n",
            "episodes: 3550\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 175625\n",
            "episodes: 3560\n",
            "mean 100 episode reward: 6.6\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 176257\n",
            "episodes: 3570\n",
            "mean 100 episode reward: 6.9\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 176953\n",
            "episodes: 3580\n",
            "mean 100 episode reward: 7.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 177629\n",
            "episodes: 3590\n",
            "mean 100 episode reward: 7.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 178126\n",
            "episodes: 3600\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 178760\n",
            "episodes: 3610\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 179429\n",
            "episodes: 3620\n",
            "mean 100 episode reward: 7.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 180006\n",
            "episodes: 3630\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 180785\n",
            "episodes: 3640\n",
            "mean 100 episode reward: 7.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 181562\n",
            "episodes: 3650\n",
            "mean 100 episode reward: 7.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 182118\n",
            "episodes: 3660\n",
            "mean 100 episode reward: 7.3\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 182633\n",
            "episodes: 3670\n",
            "mean 100 episode reward: 7.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 183270\n",
            "episodes: 3680\n",
            "mean 100 episode reward: 7.0\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 183896\n",
            "episodes: 3690\n",
            "mean 100 episode reward: 6.9\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 184292\n",
            "episodes: 3700\n",
            "mean 100 episode reward: 6.8\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 184860\n",
            "episodes: 3710\n",
            "mean 100 episode reward: 6.9\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 185711\n",
            "episodes: 3720\n",
            "mean 100 episode reward: 7.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 186161\n",
            "episodes: 3730\n",
            "mean 100 episode reward: 7.1\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 186794\n",
            "episodes: 3740\n",
            "mean 100 episode reward: 6.9\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 187231\n",
            "episodes: 3750\n",
            "mean 100 episode reward: 6.5\n",
            "% time spent exploring: 1\n",
            "********************************************************\n",
            "********************************************************\n",
            "steps: 187747\n",
            "episodes: 3760\n",
            "mean 100 episode reward: 6.4\n",
            "% time spent exploring: 1\n",
            "********************************************************\n"
          ]
        }
      ]
    }
  ]
}