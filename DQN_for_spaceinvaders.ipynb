{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNENJaUU3Lbefld4QFSQEKd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akiran703/DQN_demo_for_SI/blob/main/DQN_for_spaceinvaders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLDMTBoYemAq",
        "outputId": "5c3a3dee-572c-47f7-c4b7-1300f9ba40fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Requirement already satisfied: ale-py~=0.7.5 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.7.5)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.4.0)\n",
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.10/dist-packages (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (8.1.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (2.31.0)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.10/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autorom[accept-rom-license]) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "from gym import spaces\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "cXIduZz6emHX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ale_py import ALEInterface\n",
        "ale = ALEInterface()\n",
        "\n",
        "from ale_py.roms import SpaceInvaders\n",
        "ale.loadROM(SpaceInvaders)"
      ],
      "metadata": {
        "id": "6V0RZfjye4sI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in order to reduce instability for when the Deep Q-learning is being trained, we apply a replay system to help.\n",
        "#A replay system essentially is a way to store transistions that the agent observes and it can be used later training.\n",
        "#This helps with reducing catostraphic forgetting in which the agent forgets all previous experiences as its new experience and reduces correlation between experiences.\n",
        "\n",
        "\n",
        "\n",
        "#creating that class that creates the Replay system\n",
        "class ReplayMemory:\n",
        "    #constructor creating a list to store the experiences\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = []\n",
        "        self.maxlen=capacity\n",
        "        self.nextindex = 0\n",
        "\n",
        "    # we can not keep every single transition, so we will remove the oldest one\n",
        "    def push(self, state,action,reward,next_state,done):\n",
        "\n",
        "      data = (state, action, reward, next_state, done)\n",
        "\n",
        "      if self.nextindex >= len(self.memory):\n",
        "        self.memory.append(data)\n",
        "      else:\n",
        "        self.memory[self.nextindex] = data\n",
        "      self.nextindex = (self.nextindex + 1) % self.maxlen\n",
        "\n",
        "\n",
        "    #encode the sample into arrays\n",
        "    def encode_sample(self,indices):\n",
        "      states, actions, rewards, next_states, dones = [], [], [], [], []\n",
        "      for i in indices:\n",
        "        d = self.memory[i]\n",
        "        state, action, reward, next_state, done = d\n",
        "        states.append(np.array(state,copy=False))\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "        next_states.append(next_state,copy=False)\n",
        "        dones.append(done)\n",
        "      return np.array(states), np.array(actions), np.array(rewards), np.array(next_states), np.array(dones)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # getting a random experience\n",
        "    def sample(self, batch_size):\n",
        "      indx = np.random.randint(0, len(self.memory) - 1, size = batch_size)\n",
        "      return self.encode_sample(indx)\n",
        "\n",
        "\n",
        "    # the length of the memory\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n"
      ],
      "metadata": {
        "id": "s7iZ3HHtw90C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "  def __init__(self,observation_space: spaces.Box, action_space: spaces.discrete):\n",
        "     super().__init__()\n",
        "     # checks to enable that the observation space is a box, it forms a channels x widith x height, the action space is discrete\n",
        "     assert type(observation_space) == spaces.Box\n",
        "     assert type(observation_space.shape) == 3\n",
        "     assert type(action_space) == spaces.Discrete\n",
        "\n",
        "      #Convolution layer with activation function\n",
        "     self.conv = nn.Sequential(\n",
        "         nn.Conv2d(in_channels=observation_space.shape[0], out_channels=32, kernel_size=8, stride=4),\n",
        "         nn.ReLU(),\n",
        "         nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "         nn.ReLU(),\n",
        "         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "         nn.ReLU()\n",
        "     )\n",
        "\n",
        "     #fully connected layer\n",
        "     self.fc = nn.Sequential(\n",
        "            nn.Linear(in_features=64*7*7 , out_features=512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=512, out_features=action_space.n)\n",
        "        )\n",
        "  #feed foward neural network\n",
        "  def forward(self, x):\n",
        "        conv_out = self.conv(x).view(x.size()[0],-1)\n",
        "        return self.fc(conv_out)"
      ],
      "metadata": {
        "id": "AXtroIYrw92m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating the agent class which contains double DQN and\n",
        "\n",
        "class DQN_Agent:\n",
        "  def __init__(self,observation_space: spaces.Box, action_space: spaces.discrete,replay_buffer: ReplayMemory,use_double_dqn,learning_rate,batch_size,gamma,device=torch.device('cpu')):\n",
        "    self.memory = replay_buffer\n",
        "    #double dqn to make sure the estimated Q-value isnt overestimated\n",
        "    self.use_double_dqn = use_double_dqn\n",
        "    self.batch_size = batch_size\n",
        "    self.gamma = gamma\n",
        "\n",
        "\n",
        "    #the network to decide the best policy to take (highest Q-value)\n",
        "    self.policy_network = DQN(observation_space,action_space).to(device)\n",
        "    #the network to calculate out Q-value of taking that action in the next state\n",
        "    self.target_network = DQN(observation_space,action_space).to(device)\n",
        "    self.update_target_network()\n",
        "    self.target_network.eval()\n",
        "\n",
        "    self.optimiser = torch.optim.RMSprop(self.policy_network.parameters(), learning_rate = learning_rate)\n",
        "\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "LChc70SSw94X"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to create an environment\n",
        "\n",
        "def create_env():\n",
        "  base_env = gym.make(\"ALE/SpaceInvaders-v5\")\n",
        "  env = gym.wrappers.GrayScaleObservation(base_env)\n",
        "  env = gym.wrappers.FrameStack(env,4)\n",
        "  env = gym.wrappers.ResizeObservation(env, (84,84))\n",
        "  #print(base_env.action_space)\n",
        "  #print(env.observation_space)\n",
        "\n",
        "  return env\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "real_env = create_env()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43aoarQ1hzAK",
        "outputId": "4693f93e-296b-429c-fcc5-c7692c34fa00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    }
  ]
}